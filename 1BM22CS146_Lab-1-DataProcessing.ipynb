{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiBQ57vWLWRoOr2J7dWYUt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('housing.csv')\n","\n","print(\"Information of all columns:\")\n","print(df.info())\n","\n","print(\"\\nStatistical information of all numerical columns:\")\n","print(df.describe())\n","\n","print(\"\\nCount of unique labels for 'Ocean Proximity' column:\")\n","print(df['ocean_proximity'].value_counts())\n","\n","print(\"\\nColumns with missing values:\")\n","missing_values = df.isnull().sum()\n","print(missing_values[missing_values > 0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KCS-zliP1rP","executionInfo":{"status":"ok","timestamp":1740992891820,"user_tz":-330,"elapsed":161,"user":{"displayName":"Mahika D","userId":"09914683227549507252"}},"outputId":"f2ec9601-fa3a-499e-af17-9d5dfe6faeff"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Information of all columns:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 20640 entries, 0 to 20639\n","Data columns (total 10 columns):\n"," #   Column              Non-Null Count  Dtype  \n","---  ------              --------------  -----  \n"," 0   longitude           20640 non-null  float64\n"," 1   latitude            20640 non-null  float64\n"," 2   housing_median_age  20640 non-null  float64\n"," 3   total_rooms         20640 non-null  float64\n"," 4   total_bedrooms      20433 non-null  float64\n"," 5   population          20640 non-null  float64\n"," 6   households          20640 non-null  float64\n"," 7   median_income       20640 non-null  float64\n"," 8   median_house_value  20640 non-null  float64\n"," 9   ocean_proximity     20640 non-null  object \n","dtypes: float64(9), object(1)\n","memory usage: 1.6+ MB\n","None\n","\n","Statistical information of all numerical columns:\n","          longitude      latitude  housing_median_age   total_rooms  \\\n","count  20640.000000  20640.000000        20640.000000  20640.000000   \n","mean    -119.569704     35.631861           28.639486   2635.763081   \n","std        2.003532      2.135952           12.585558   2181.615252   \n","min     -124.350000     32.540000            1.000000      2.000000   \n","25%     -121.800000     33.930000           18.000000   1447.750000   \n","50%     -118.490000     34.260000           29.000000   2127.000000   \n","75%     -118.010000     37.710000           37.000000   3148.000000   \n","max     -114.310000     41.950000           52.000000  39320.000000   \n","\n","       total_bedrooms    population    households  median_income  \\\n","count    20433.000000  20640.000000  20640.000000   20640.000000   \n","mean       537.870553   1425.476744    499.539680       3.870671   \n","std        421.385070   1132.462122    382.329753       1.899822   \n","min          1.000000      3.000000      1.000000       0.499900   \n","25%        296.000000    787.000000    280.000000       2.563400   \n","50%        435.000000   1166.000000    409.000000       3.534800   \n","75%        647.000000   1725.000000    605.000000       4.743250   \n","max       6445.000000  35682.000000   6082.000000      15.000100   \n","\n","       median_house_value  \n","count        20640.000000  \n","mean        206855.816909  \n","std         115395.615874  \n","min          14999.000000  \n","25%         119600.000000  \n","50%         179700.000000  \n","75%         264725.000000  \n","max         500001.000000  \n","\n","Count of unique labels for 'Ocean Proximity' column:\n","ocean_proximity\n","<1H OCEAN     9136\n","INLAND        6551\n","NEAR OCEAN    2658\n","NEAR BAY      2290\n","ISLAND           5\n","Name: count, dtype: int64\n","\n","Columns with missing values:\n","total_bedrooms    207\n","dtype: int64\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_df6IuXNage","executionInfo":{"status":"ok","timestamp":1740993810540,"user_tz":-330,"elapsed":3488,"user":{"displayName":"Mahika D","userId":"09914683227549507252"}},"outputId":"65e7e7a6-8ea5-4577-cfc5-efb78b478dba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/wenruliu/adult-income-dataset/versions/2\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"wenruliu/adult-income-dataset\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["import os\n","\n","# List all files in the directory to find the actual CSV file\n","directory_path = '/root/.cache/kagglehub/datasets/wenruliu/adult-income-dataset/versions/2'\n","files = os.listdir(directory_path)\n","print(files)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sed_Xj41Vejn","executionInfo":{"status":"ok","timestamp":1740994114053,"user_tz":-330,"elapsed":14,"user":{"displayName":"Mahika D","userId":"09914683227549507252"}},"outputId":"1c048886-c296-4ee1-9181-57ccea8b1ca5"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['adult.csv']\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder\n","import numpy as np\n","\n","# Load datasets\n","diab = pd.read_csv('diadata.csv')\n","adult = pd.read_csv('/root/.cache/kagglehub/datasets/wenruliu/adult-income-dataset/versions/2/adult.csv')\n","\n","# ===========================================================\n","# Diabetes Data Preprocessing\n","\n","# 1. Handle missing values for numeric columns: Use median for numerical columns\n","ncol = diab.select_dtypes(include=['float64', 'int64']).columns\n","imputer_numeric = SimpleImputer(strategy='median')\n","diab[ncol] = imputer_numeric.fit_transform(diab[ncol])\n","\n","# 2. Handle missing values for categorical columns: Use most frequent value for categorical columns\n","catcol = diab.select_dtypes(include=['object']).columns\n","imputer_categorical = SimpleImputer(strategy='most_frequent')\n","diab[catcol] = imputer_categorical.fit_transform(diab[catcol])\n","\n","# 3. Handle categorical data: Encode 'Gender' and 'CLASS' columns\n","label_encoder = LabelEncoder()\n","diab['Gender'] = label_encoder.fit_transform(diab['Gender'])\n","diab['CLASS'] = label_encoder.fit_transform(diab['CLASS'])\n","\n","# 4. Handle outliers: Cap values for 'Urea' column\n","lower, upper = diab['Urea'].quantile([0.01, 0.99])\n","diab['Urea'] = np.clip(diab['Urea'], lower, upper)\n","\n","# 5. Apply Min-Max scaling\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","diabetes_scaled_minmax = pd.DataFrame(scaler.fit_transform(diab), columns=diab.columns)\n","\n","# ===========================================================\n","# Adult Income Data Preprocessing\n","\n","# 1. Handle missing values for numeric columns: Use median for numerical columns\n","ncol_adult = adult.select_dtypes(include=['float64', 'int64']).columns\n","imputer_numeric_adult = SimpleImputer(strategy='median')\n","adult[ncol_adult] = imputer_numeric_adult.fit_transform(adult[ncol_adult])\n","\n","# 2. Handle missing values for categorical columns: Use most frequent value for categorical columns\n","catcol_adult = adult.select_dtypes(include=['object']).columns\n","imputer_categorical_adult = SimpleImputer(strategy='most_frequent')\n","adult[catcol_adult] = imputer_categorical_adult.fit_transform(adult[catcol_adult])\n","\n","# 3. Handle categorical data: Encode categorical columns\n","for col in catcol_adult:\n","    adult[col] = label_encoder.fit_transform(adult[col])\n","\n","# 4. Handle outliers: Cap values for 'age' column\n","lower, upper = adult['age'].quantile([0.01, 0.99])\n","adult['age'] = np.clip(adult['age'], lower, upper)\n","\n","# 5. Apply Min-Max scaling\n","adult_income_scaled_minmax = pd.DataFrame(scaler.fit_transform(adult), columns=adult.columns)\n","\n","# Display the first few rows of the scaled data\n","print(\"Diabetes dataset after Min-Max scaling:\")\n","print(diabetes_scaled_minmax.head())\n","\n","print(\"Adult Income dataset after Min-Max scaling:\")\n","print(adult_income_scaled_minmax.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YR45AOG-Od6X","executionInfo":{"status":"ok","timestamp":1740994418189,"user_tz":-330,"elapsed":705,"user":{"displayName":"Mahika D","userId":"09914683227549507252"}},"outputId":"7e082504-8f82-4473-96c5-b7fe7de92f29"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Diabetes dataset after Min-Max scaling:\n","         ID  No_Pation  Gender       AGE      Urea        Cr     HbA1c  \\\n","0  0.627034   0.000237     0.0  0.508475  0.143617  0.050378  0.264901   \n","1  0.918648   0.000452     0.5  0.101695  0.132979  0.070529  0.264901   \n","2  0.524406   0.000634     0.0  0.508475  0.143617  0.050378  0.264901   \n","3  0.849812   0.001160     0.0  0.508475  0.143617  0.050378  0.264901   \n","4  0.629537   0.000452     0.5  0.220339  0.271277  0.050378  0.264901   \n","\n","       Chol        TG       HDL       LDL      VLDL       BMI  CLASS  \n","0  0.407767  0.044444  0.226804  0.114583  0.011461  0.173913    0.0  \n","1  0.359223  0.081481  0.092784  0.187500  0.014327  0.139130    0.0  \n","2  0.407767  0.044444  0.226804  0.114583  0.011461  0.173913    0.0  \n","3  0.407767  0.044444  0.226804  0.114583  0.011461  0.173913    0.0  \n","4  0.475728  0.051852  0.061856  0.177083  0.008596  0.069565    0.0  \n","Adult Income dataset after Min-Max scaling:\n","        age  workclass    fnlwgt  education  educational-num  marital-status  \\\n","0  0.140351       0.50  0.145129   0.066667         0.400000        0.666667   \n","1  0.368421       0.50  0.052451   0.733333         0.533333        0.333333   \n","2  0.192982       0.25  0.219649   0.466667         0.733333        0.333333   \n","3  0.473684       0.50  0.100153   1.000000         0.600000        0.333333   \n","4  0.017544       0.00  0.061708   1.000000         0.600000        0.666667   \n","\n","   occupation  relationship  race  gender  capital-gain  capital-loss  \\\n","0    0.500000           0.6   0.5     1.0      0.000000           0.0   \n","1    0.357143           0.0   1.0     1.0      0.000000           0.0   \n","2    0.785714           0.0   1.0     1.0      0.000000           0.0   \n","3    0.500000           0.0   0.5     1.0      0.076881           0.0   \n","4    0.000000           0.6   1.0     0.0      0.000000           0.0   \n","\n","   hours-per-week  native-country  income  \n","0        0.397959         0.95122     0.0  \n","1        0.500000         0.95122     0.0  \n","2        0.397959         0.95122     1.0  \n","3        0.397959         0.95122     1.0  \n","4        0.295918         0.95122     0.0  \n"]}]},{"cell_type":"markdown","source":["Min-Max Scaling scales the data to a fixed range, typically [0, 1], by subtracting the minimum and dividing by the range. It is sensitive to outliers since they can affect the min and max values. It’s ideal for algorithms that require data in a specific range, such as neural networks and k-nearest neighbors (KNN).\n","\n","Standardization, on the other hand, transforms the data by subtracting the mean and dividing by the standard deviation, resulting in a distribution with a mean of 0 and a standard deviation of 1. It’s less sensitive to outliers compared to Min-Max scaling and is preferred for algorithms that assume normally distributed data, like linear regression, logistic regression, and PCA. Standardization works better when the data has outliers or is not bounded."],"metadata":{"id":"AuGzMUeHe_aW"}}]}